---
title: 模型误差与过拟合
categories:
  - 计算机
  - 机器学习
mathjax: true
abbrlink: 12469
date: 2022-07-29 10:20:59
---
误差分解
$$
E(\text{error})=\text{bias}^2+\text{Var}+\varepsilon.
$$

<!--more-->

---

先定义:

均方误差
$$
\text{MSE}(\hat\theta)=E[(\hat\theta-\theta)^2]
$$
偏差
$$
\text{bias}(\hat\theta,\theta)=E[\hat\theta]-\theta
$$

那么
$$
\begin{align*}
\text{MSE}(\hat\theta)&=E\left[\left(\hat\theta-\theta\right)^2\right]\\
&=E\left[\left(\hat\theta-E[\hat\theta]+E[\hat\theta]-\theta\right)^2\right]\\
&=E\left[\left(\hat\theta-E[\hat\theta]\right)^2+2\left(\hat\theta-E[\hat\theta]\right)\left(E[\hat\theta]-\theta\right)+\left(E[\hat\theta]-\theta\right)^2\right]\\
&=E\left[\left(\hat\theta-E[\hat\theta]\right)^2\right]+2E\left[\left(\hat\theta-E[\hat\theta]\right)\left(E[\hat\theta]-\theta\right)\right]+E\left[\left(E[\hat\theta]-\theta\right)^2\right]\\
&=\text{Var}(\hat\theta)+2\left(E[\hat\theta]-\theta\right)E\left[\left(\hat\theta-E[\hat\theta]\right)\right]+\left(E[\hat\theta]-\theta\right)^2\\
&=\text{Var}+\text{bias}^2
\end{align*}
$$

随着模型复杂度的增加, bias 减少, variance 增加. 也就是说, 能力变强, 不稳定性变大. 那么总误差呈现一个先减后增的趋势:
<img src='error.png'>
这就是 bias-variance dilemma.